{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "import pyodbc\n",
    "from collections import OrderedDict\n",
    "from itertools import product\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rfm_query(typ, count=1000):\n",
    "    query = \"\"\"\n",
    "            select {top} c.Id CustId\n",
    "                ,{field}\n",
    "\n",
    "                from directcrm.Customers c\n",
    "                    join directcrm.CustomerActions ca on c.Id = ca.CustomerId\n",
    "                    join directcrm.RetailOrders ro on ca.Id = ro.FirstCustomerActionId\n",
    "                    join directcrm.RetailOrderHistory roh on ro.Id = roh.OrderId\n",
    "                    join directcrm.RetailPurchaseHistory rph on roh.id = rph.OrderHistoryItemId\n",
    "                    join directcrm.RetailPurchaseStatuses rps on rph.StatusId = rps.Id\n",
    "\n",
    "                where rps.CategorySystemName not in ('Returned','Cancelled','InCart') and roh.IsCurrentOtherwiseNull = 1\n",
    "\n",
    "                group by c.Id\n",
    "\n",
    "                order by c.Id\n",
    "            \"\"\"\n",
    "    \n",
    "    fields = {'r': 'DateDiff(hh, Max(ca.DateTimeUtc), GetDate()) Recency_hours',\n",
    "              'f': 'Count(ca.Id) CountActions',\n",
    "              'm': 'Sum(roh.EffectivePayedAmount) Amount'}\n",
    "    \n",
    "    top = ''\n",
    "    if count is not None:\n",
    "        top = \"top({})\".format(count)\n",
    "    \n",
    "    query = query.format(top=top, field=fields[typ])\n",
    "    return query\n",
    "\n",
    "\n",
    "def get_rfm_data(cnxn, typ, count=1000):\n",
    "    cursor = cnxn.cursor()\n",
    "    cursor.execute(get_rfm_query(typ, count))\n",
    "    rows = list(map(lambda t: [t[0], t[1]], cursor.fetchall()))\n",
    "    df = pd.DataFrame(rows, columns=['id', 'rfm_{}'.format(typ)])\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_full_rfm_data(count = 100): \n",
    "    cnxn = pyodbc.connect(driver='{SQL Server}',\n",
    "                          server='aurora',\n",
    "                          database='hoff')\n",
    "    data_r = get_rfm_data(cnxn, 'r', count=count)\n",
    "    data_f = get_rfm_data(cnxn, 'f', count=count)\n",
    "    data_m = get_rfm_data(cnxn, 'm', count=count)\n",
    "    cnxn.close()\n",
    "    \n",
    "    if sum (data_r.id != data_f.id) or sum(data_r.id != data_m.id):\n",
    "        raise RuntimeError(\"Не совпадают индексы\")\n",
    "    else:\n",
    "        data = pd.concat([data_r, data_f.drop('id', axis=1), data_m.drop('id', axis=1)], axis=1)\n",
    "    data['rfm_m'] = data['rfm_m'].astype(float)\n",
    "    return data\n",
    "\n",
    "\n",
    "def reduce_tails(data, min_q=0.025, max_q=0.975):\n",
    "    filtered_data = data[(data['rfm_r'] >= data['rfm_r'].quantile(min_q)) &\n",
    "                         (data['rfm_r'] <= data['rfm_r'].quantile(max_q)) &\n",
    "                         (data['rfm_f'] >= data['rfm_f'].quantile(min_q)) &\n",
    "                         (data['rfm_f'] <= data['rfm_f'].quantile(max_q)) &\n",
    "                         (data['rfm_m'] >= data['rfm_m'].quantile(min_q)) &\n",
    "                         (data['rfm_m'] <= data['rfm_m'].quantile(max_q))]\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def draw_3d_plot(data, labels=None):\n",
    "    fig = plt.figure()\n",
    "    ax = Axes3D(fig)\n",
    "    \n",
    "    ax.set_xlabel(\"R\")\n",
    "    ax.set_ylabel(\"F\")\n",
    "    ax.set_zlabel(\"M\")\n",
    "    \n",
    "    if labels is None:\n",
    "        x = list(data['rfm_r'])\n",
    "        y = list(data['rfm_f'])\n",
    "        z = list(data['rfm_m'])\n",
    "        ax.scatter(x, y, z)\n",
    "        \n",
    "    else:\n",
    "        for n in np.unique(labels):\n",
    "            indices = labels == n\n",
    "            x = list(data.loc[indices, 'rfm_r'])\n",
    "            y = list(data.loc[indices, 'rfm_f'])\n",
    "            z = list(data.loc[indices, 'rfm_m'])\n",
    "\n",
    "            c = [[n * 2 % 256, n * 5 % 256, n * 13 % 256]]\n",
    "            ax.scatter(x, y, z)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = get_full_rfm_data(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losted: 4.300000000000004%\n"
     ]
    }
   ],
   "source": [
    "percent_to_drop = 2\n",
    "reduced_data = reduce_tails(raw_data.copy(), percent_to_drop / 2 / 100, 1 - percent_to_drop / 2 / 100)\n",
    "print('Losted: {}%'.format((1 - reduced_data.shape[0] / raw_data.shape[0]) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = reduced_data\n",
    "X = data[['rfm_r', 'rfm_f', 'rfm_m']]\n",
    "X_scaled_std = pd.DataFrame(StandardScaler().fit_transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw_3d_plot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#draw_3d_plot(X_scaled_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритмы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Решено использовать Standard Scaling (считать только с ним!)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выбранные алгоритмы\n",
    "\n",
    "1. **K-means** - классика, без него никуда.\n",
    "Не слишком быстрый, зато простой и понятный.\n",
    "*Достоинства*: кластеры выпуклые\n",
    "\n",
    "2. **Agglomerative clustering** - хороший алгоритм, есть 3 варианта опции \"связь\". Быстрый.\n",
    "*Достоинства*: Гораздо умнее, чем k-means\n",
    "*Недостатки*: невыпуклые кластеры -> будем следить, чтобы ложились в параллелепипеды\n",
    "\n",
    "2. **Mean-shift** - Неплохо работает.  \n",
    "*Недостатки*: Нельзя задать число кластеров -> будем следить, чтобы их число было в заданных пределах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики, не требующие знания истинных меток классов\n",
    "*Пока рассматриваем только их*\n",
    "\n",
    "1. **Silhouette Coefficient** - силуэт. От -1 до +1.\n",
    "\n",
    "2. **Calinski-Harabaz Index** - быстро считается"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabaz_score\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, MeanShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning(X, params, clr, rs=None):\n",
    "    names = tuple(params.keys())\n",
    "    values_lists = list(params.values())\n",
    "    combs = list(product(*values_lists))\n",
    "    \n",
    "    results = {'name': clr.__name__, 'params': names}\n",
    "    results['results'] = OrderedDict()\n",
    "    for comb in combs:\n",
    "        kwargs = {name: comb[i] for i, name in enumerate(names)}\n",
    "        labels = clr(**kwargs).fit(X).labels_\n",
    "        u_lbs_num = len(np.unique(labels))\n",
    "        if u_lbs_num > 1:\n",
    "            m1 = silhouette_score(X, labels, random_state=rs)\n",
    "            m2 = calinski_harabaz_score(X, labels)\n",
    "        else:\n",
    "            m1 = -1\n",
    "            m2 = -1\n",
    "        results['results'][comb] = {'labels': labels, 'm1': m1, 'm2': m2}\n",
    "    \n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best_result(results):\n",
    "    combs = list(results.keys())\n",
    "    m1 = [d['m1'] for d in results.values()]\n",
    "    m2 = [d['m1'] for d in results.values()]\n",
    "    lbs = [d['labels'] for d in results.values()]\n",
    "    \n",
    "    best_1, best_2 = np.argmax(m1), np.argmax(m2)\n",
    "    best_comb_1, best_comb_2 = combs[best_1], combs[best_2]\n",
    "    best_res_1, best_res_2 = m1[best_1], m2[best_2]\n",
    "    best_lbs_1, best_lbs_2 = lbs[best_1], lbs[best_2]\n",
    "    return {'m1': {'comb': best_comb_1, 'res': best_res_1, 'labels': best_lbs_1},\n",
    "            'm2': {'comb': best_comb_2, 'res': best_res_2, 'labels': best_lbs_2}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_bests(X, n_min, n_max, rs=None, n_jobs=-3):\n",
    "    \"\"\"rs = random_state\"\"\"\n",
    "    \n",
    "    reports = []\n",
    "    \n",
    "    # KMeans\n",
    "    print('KMeans')\n",
    "    params_kmeans = {'n_clusters': list(range(n_min, n_max + 1)),\n",
    "                     'random_state': [rs],\n",
    "                     'n_jobs': [n_jobs]}\n",
    "    reports.append(tuning(X, params_kmeans, KMeans, rs=rs))\n",
    "    \n",
    "    # Agglomerative Clustering\n",
    "    print('Agglomerative Clustering')\n",
    "    params_agglmrt = {'n_clusters': list(range(n_min, n_max + 1)),\n",
    "                      'linkage': ['ward', 'average'],\n",
    "                      'affinity': ['euclidean']}\n",
    "    reports.append(tuning(X, params_agglmrt, AgglomerativeClustering, rs=rs))\n",
    "    \n",
    "    # Mean-shift\n",
    "    print('Mean-shift')\n",
    "    params_meanshift = {'cluster_all': [True, False], \n",
    "                        'n_jobs': [n_jobs]}\n",
    "    reports.append(tuning(X_scaled_std, params_meanshift, MeanShift, rs=rs))\n",
    "     \n",
    "    #import ipdb; ipdb.set_trace()\n",
    "    # Filtering\n",
    "    print('Filtering')\n",
    "    n_min_ext, n_max_ext = n_min - (n_max - n_min) // 3, n_max + (n_max - n_min) // 3\n",
    "    for i, rep in enumerate(reports):\n",
    "        bad_combs = []\n",
    "        for comb, res in rep['results'].items():\n",
    "            u_lbs_num = len(np.unique(res['labels']))\n",
    "            if not n_min_ext <= u_lbs_num <= n_max_ext:\n",
    "                bad_combs.append(comb)\n",
    "        for comb in bad_combs:\n",
    "            del reports[i]['results'][comb]\n",
    "        if len(rep['results']) == 0:\n",
    "            del reports[i]\n",
    "            \n",
    "    # Choousing best\n",
    "    print('Choousing best')\n",
    "    bests = OrderedDict()\n",
    "    for i, rep in enumerate(reports):\n",
    "        bests[rep['name']] = {'params': rep['params'], \n",
    "                              'bests': choose_best_result(rep['results'])}\n",
    "    \n",
    "    return bests    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans\n",
      "Agglomerative Clustering\n",
      "Mean-shift\n",
      "Filtering\n",
      "Choousing best\n"
     ]
    }
   ],
   "source": [
    "rs=17\n",
    "n_min, n_max = 8, 14\n",
    "bests = choose_bests(X, n_min, n_max, rs=rs, n_jobs=-3)\n",
    "bests_ = deepcopy(bests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('KMeans',\n",
       "              {'bests': {'m1': {'comb': (8, 17, -3),\n",
       "                 'labels': array([0, 0, 6, ..., 0, 0, 6]),\n",
       "                 'res': 0.6427855587540297},\n",
       "                'm2': {'comb': (8, 17, -3),\n",
       "                 'labels': array([0, 0, 6, ..., 0, 0, 6]),\n",
       "                 'res': 0.6427855587540297}},\n",
       "               'params': ('n_clusters', 'random_state', 'n_jobs')}),\n",
       "             ('AgglomerativeClustering',\n",
       "              {'bests': {'m1': {'comb': (9, 'average', 'euclidean'),\n",
       "                 'labels': array([0, 0, 0, ..., 0, 0, 0], dtype=int64),\n",
       "                 'res': 0.7558637568668873},\n",
       "                'm2': {'comb': (9, 'average', 'euclidean'),\n",
       "                 'labels': array([0, 0, 0, ..., 0, 0, 0], dtype=int64),\n",
       "                 'res': 0.7558637568668873}},\n",
       "               'params': ('n_clusters', 'linkage', 'affinity')})])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.view_init(30, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
